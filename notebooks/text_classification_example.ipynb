{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "text_classification_example.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/rdenadai/BolaoSimples/blob/master/notebooks/text_classification_example.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "WGxO__uG47K5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## An√°lise e Valida√ß√£o de Textos em Portugu√™s\n"
      ]
    },
    {
      "metadata": {
        "id": "AzVNkORP5dvZ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Refer√™ncias:\n",
        "\n",
        " - [NLTK](http://www.nltk.org/howto/portuguese_en.html)\n",
        " - [spaCy](https://spacy.io/usage/spacy-101)\n",
        " - [Utilizando processamento de linguagem natural para criar uma sumariza√ß√£o autom√°tica de textos](https://medium.com/@viniljf/utilizando-processamento-de-linguagem-natural-para-criar-um-sumariza%C3%A7%C3%A3o-autom%C3%A1tica-de-textos-775cb428c84e)\n",
        " - [Latent Semantic Analysis (LSA) for Text Classification Tutorial](http://mccormickml.com/2016/03/25/lsa-for-text-classification-tutorial/)\n",
        " - [Topic Modeling with LSA, PLSA, LDA & lda2Vec](https://medium.com/nanonets/topic-modeling-with-lsa-psla-lda-and-lda2vec-555ff65b0b05)\n",
        " - [Unsupervised Emotion Detection from Text using Semantic and Syntactic Relations](http://www.cse.yorku.ca/~aan/research/paper/Emo_WI10.pdf)"
      ]
    },
    {
      "metadata": {
        "id": "fJl2_xgi5M4C",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Instala√ß√£o"
      ]
    },
    {
      "metadata": {
        "id": "6gSt861Txvl1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 716
        },
        "outputId": "d2ba7456-2664-4d4b-860f-ebca60affde7"
      },
      "cell_type": "code",
      "source": [
        "!pip install -U spacy\n",
        "!python -m spacy download en\n",
        "!python -m spacy download pt\n",
        "# !pip install feedparser"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.0.12)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.6)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.28.0)\n",
            "Requirement already satisfied, skipping upgrade: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.31.2)\n",
            "Requirement already satisfied, skipping upgrade: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.1)\n",
            "Requirement already satisfied, skipping upgrade: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy) (6.10.3)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.9.6)\n",
            "Requirement already satisfied, skipping upgrade: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.35)\n",
            "Requirement already satisfied, skipping upgrade: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.2.8.2)\n",
            "Requirement already satisfied, skipping upgrade: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy) (2017.4.5)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n",
            "Requirement already satisfied, skipping upgrade: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.5.6)\n",
            "Requirement already satisfied, skipping upgrade: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.4.4.1)\n",
            "Requirement already satisfied, skipping upgrade: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (0.9.0.1)\n",
            "Requirement already satisfied, skipping upgrade: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.10.11)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.26.0)\n",
            "Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n",
            "Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.8.24)\n",
            "Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n",
            "Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "\n",
            "    You can now load the model via spacy.load('en')\n",
            "\n",
            "Requirement already satisfied: pt_core_news_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/pt_core_news_sm-2.0.0/pt_core_news_sm-2.0.0.tar.gz#egg=pt_core_news_sm==2.0.0 in /usr/local/lib/python3.6/dist-packages (2.0.0)\n",
            "\n",
            "\u001b[93m    Linking successful\u001b[0m\n",
            "    /usr/local/lib/python3.6/dist-packages/pt_core_news_sm -->\n",
            "    /usr/local/lib/python3.6/dist-packages/spacy/data/pt\n",
            "\n",
            "    You can now load the model via spacy.load('pt')\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PcT7xbKKo55-",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        },
        "outputId": "dfe18ec7-2466-4188-b7aa-575138b29e5d"
      },
      "cell_type": "code",
      "source": [
        "# Download Oplexicon\n",
        "!rm -rf wget-log*\n",
        "!rm -rf oplexicon_v3.0\n",
        "!wget -O oplexicon_v3.0.zip https://github.com/rdenadai/sentiment-analysis-2018-president-election/blob/master/dataset/oplexicon_v3.0.zip?raw=true\n",
        "!unzip oplexicon_v3.0.zip\n",
        "!ls -lh"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Redirecting output to ‚Äòwget-log‚Äô.\n",
            "Archive:  oplexicon_v3.0.zip\n",
            "  inflating: oplexicon_v3.0/lexico_v3.0.txt  \n",
            "  inflating: oplexicon_v3.0/README   \n",
            "total 120K\n",
            "drwxr-xr-x 2 root root 4.0K Oct  5 20:06 oplexicon_v3.0\n",
            "-rw-r--r-- 1 root root 102K Oct  5 20:06 oplexicon_v3.0.zip\n",
            "drwxr-xr-x 2 root root 4.0K Sep 28 23:32 sample_data\n",
            "-rw-r--r-- 1 root root 1.6K Oct  5 20:06 wget-log\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mVxM4zTo5QSB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ]
    },
    {
      "metadata": {
        "id": "HLlEa6uEyX11",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "6a962311-abf7-4a65-9a9e-ae235a31f8f1"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('rslp')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('floresta')\n",
        "nltk.download('mac_morpho')\n",
        "nltk.download('machado')\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('words')\n",
        "\n",
        "import concurrent.futures\n",
        "import codecs\n",
        "import re\n",
        "import pprint\n",
        "from random import shuffle\n",
        "from string import punctuation\n",
        "import copy\n",
        "\n",
        "import numpy as np\n",
        "from scipy.sparse.linalg import svds\n",
        "from scipy.linalg import svd\n",
        "import pandas as pd\n",
        "import spacy\n",
        "\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity, cosine_distances\n",
        "from sklearn.utils.extmath import randomized_svd\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.corpus import floresta as flt\n",
        "from nltk.corpus import machado as mch\n",
        "from nltk.corpus import mac_morpho as mcm\n",
        "\n",
        "\n",
        "nlp = spacy.load('pt')\n",
        "pp = pprint.PrettyPrinter(indent=4)\n",
        "stemmer = nltk.stem.RSLPStemmer()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]   Package rslp is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]   Package floresta is already up-to-date!\n",
            "[nltk_data] Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]   Package mac_morpho is already up-to-date!\n",
            "[nltk_data] Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]   Package machado is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Z0CGK0ftshPG",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Functions"
      ]
    },
    {
      "metadata": {
        "id": "pOMbwFd3sd31",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def load_oplexicon_data(filename):\n",
        "    spacy_conv = {\n",
        "        'adj': 'ADJ',\n",
        "        'n': 'NOUN',\n",
        "        'vb': 'VERB',\n",
        "        'det': 'DET',\n",
        "        'emot': 'EMOT',\n",
        "        'htag': 'HTAG'\n",
        "    }\n",
        "    \n",
        "    data = {}\n",
        "    with codecs.open(filename, 'r', 'UTF-8') as hf:\n",
        "        lines = hf.readlines()\n",
        "        for line in lines:\n",
        "            info = line.lower().split(',')\n",
        "            if len(info[0].split()) <= 1:\n",
        "                info[1] = [spacy_conv.get(tag) for tag in info[1].split()]\n",
        "                word, tags, sent = info[:3]\n",
        "                if 'HTAG' not in tags and 'EMOT' not in tags:\n",
        "                    word = nlp(word.lower())[0].lemma_\n",
        "                    # word = word.replace('-se', '')\n",
        "                    # stem = stemmer.stem(word)\n",
        "                    if word in data:\n",
        "                        data[word] += [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "                    else:\n",
        "                        data[word] = [{\n",
        "                            'word': [word],\n",
        "                            'tags': tags,\n",
        "                            'sentiment': sent\n",
        "                        }]\n",
        "    return data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FwSQhPMk5Scp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Usage"
      ]
    },
    {
      "metadata": {
        "id": "mQzbtaNhy5Y4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 303
        },
        "outputId": "7e4077d7-e4f6-456a-99bb-2eb9d5560e30"
      },
      "cell_type": "code",
      "source": [
        "frase = u\"Gostaria de saber mais informa√ß√µes sobre a Amazon. Uma excelente loja de produtos online!\".lower()\n",
        "doc = nlp(frase)\n",
        "pp.pprint([(w.text, w.pos_) for w in doc])\n",
        "\n",
        "# for dc in doc:\n",
        "#     if dc.pos_ == 'VERB':\n",
        "#         print(dc.lemma_)\n",
        "#     else:\n",
        "#         print(dc)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[   ('gostaria', 'VERB'),\n",
            "    ('de', 'ADP'),\n",
            "    ('saber', 'VERB'),\n",
            "    ('mais', 'DET'),\n",
            "    ('informa√ß√µes', 'NOUN'),\n",
            "    ('sobre', 'ADP'),\n",
            "    ('a', 'DET'),\n",
            "    ('amazon', 'NOUN'),\n",
            "    ('.', 'PUNCT'),\n",
            "    ('uma', 'DET'),\n",
            "    ('excelente', 'ADJ'),\n",
            "    ('loja', 'NOUN'),\n",
            "    ('de', 'ADP'),\n",
            "    ('produtos', 'NOUN'),\n",
            "    ('online', 'ADJ'),\n",
            "    ('!', 'PUNCT')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F--aytOy3gL_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1190
        },
        "outputId": "bca89a06-aad9-48f3-e630-2b8ceac15955"
      },
      "cell_type": "code",
      "source": [
        "opx = load_oplexicon_data('oplexicon_v3.0/lexico_v3.0.txt')\n",
        "print('Oplexicon size: ', len(opx))\n",
        "print('Examples: ')\n",
        "\n",
        "view = opx.items()\n",
        "pp.pprint(list(view)[:7])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-16-aefe32c163c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mopx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_oplexicon_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'oplexicon_v3.0/lexico_v3.0.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Oplexicon size: '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Examples: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mview\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-5becb09119bb>\u001b[0m in \u001b[0;36mload_oplexicon_data\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     18\u001b[0m                 \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m'HTAG'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m'EMOT'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtags\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m                     \u001b[0mword\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlemma_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m                     \u001b[0;31m# word = word.replace('-se', '')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m                     \u001b[0;31m# stem = stemmer.stem(word)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable)\u001b[0m\n\u001b[1;32m    350\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'__call__'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE003\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomponent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m             \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mproc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE005\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32mpipeline.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.Tagger.__call__\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32mpipeline.pyx\u001b[0m in \u001b[0;36mspacy.pipeline.Tagger.predict\u001b[0;34m()\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         '''\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/api.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(seqs_in)\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    292\u001b[0m         \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseqs_in\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseqs_in\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/model.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    159\u001b[0m             \u001b[0mMust\u001b[0m \u001b[0mmatch\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m         '''\n\u001b[0;32m--> 161\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpipe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/check.py\u001b[0m in \u001b[0;36mchecked_function\u001b[0;34m(wrapped, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    144\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mExpectedTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'Callable'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m                 \u001b[0mcheck\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfix_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0marg_check_adder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/thinc/neural/_classes/softmax.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, input__BI)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mcheck\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhas_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'nB'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'nI'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0moutput__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maffine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput__BI\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0moutput__BO\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput__BO\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput__BO\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "AcajLBiFm3N1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ALEGRIA = ['abundante', 'acalmar', 'aceit√°vel', 'aclamar', 'aconchego', 'ades√£o', 'admirar', 'adorar', 'af√°vel', 'afei√ß√£o', 'afeto', 'afortunado', 'agradar', 'ajeitar', 'al√≠vio', 'amabilidade', 'amado', 'amar', 'am√°vel', 'amenizar', 'ameno', 'amig√°vel', 'amistoso', ' amizade', ' amor', ' anima√ß√£o', ' √¢nimo', 'anseio', '√¢nsia', 'ansioso', 'apaixonado', 'apaziguar', 'aplausos', 'apoiar', 'aprazer', 'apreciar', 'aprova√ß√£o', 'aproveitar', 'ardor', 'armirar', 'arrumar', 'atra√ß√£o', 'atraente', 'atrair', 'avidamente', 'avidez', '√°vido', 'belo', 'bem-estar', 'benefic√™ncia', 'beneficiador', 'benef√≠cio', 'ben√©fico', 'benevoc√™ncia', 'benignamente', 'ben√≠gno', 'bom', 'bondade', 'bondoso', 'bonito', 'brilhante', 'brincadeira', 'calma', 'calor', 'caridade', 'caridoso', 'carinho', 'cativar', 'charme', 'cheery', 'clamar', 'cofortar', 'coleguismo', 'com√©dia', 'c√¥mico', 'comover', 'compaix√£o', 'companheirismo', 'compatibilidade', 'compat√≠vel', 'complac√™ncia', 'completar', 'compreens√£o', 'conclus√£o', 'concretiza√ß√£o', 'condescend√™ncia', 'confian√ßa', 'confortante', 'congratula√ß√£o', 'conquistar', 'consentir', 'considera√ß√£o', 'consola√ß√£o', 'contentamento', 'coragem', 'cordial', 'considerar', 'consolo', 'contente', 'cuidadoso', 'cumplicidade', 'dedica√ß√£o', 'deleitado', 'delicadamente', 'delicadeza', 'delicado', 'desejar', 'despreocupa√ß√£o', 'devo√ß√£o', 'devoto', 'divers√£o', 'divertido', 'encantar', 'elogiado', 'emo√ß√£o', 'emocionante', 'emotivo', 'empatia', 'emp√°tico', 'empolga√ß√£o', 'enamorar', 'encantado', 'encorajado', 'enfeitar', 'engra√ßado', 'entendimento', 'entusiasmadamente', 'entusi√°stico', 'esperan√ßa', 'esplendor', 'estima', 'estimar', 'estimulante', 'euforia', 'euf√≥rico', 'euforizante', 'exaltar', 'excelente', 'excitar', 'expansivo', 'extasiar', 'exuberante', 'exultar', 'f√£', 'facilitar', 'familiaridade', 'fascina√ß√£o', 'fasc√≠nio', 'favor', 'favorecer', 'favorito', 'felicidade', 'feliz', 'festa', 'festejar', 'festivo', 'fidelidade', 'fiel', 'filantropia', 'filantr√≥pico', 'fraterno', 'ganhar', 'generosidade', 'generoso', 'gentil', 'gl√≥ria', 'glorificar', 'gostar', 'gostoso', 'gozar', 'gratificante', 'grato', 'hilariante', 'honra', 'humor', 'impressionar', 'incentivar', 'incentivo', 'inclina√ß√£o', 'incr√≠vel', 'inspirar', 'interessar', 'interesse', 'irmandade', 'jovial', 'jubilante', 'j√∫bilo', 'lealdade', 'leg√≠timo', 'leveza', 'louvar', 'louv√°vel', 'louvavelmente', 'lucrativo', 'lucro', 'maravilhoso', 'melhor', 'obter', 'obteve', 'ode', 'orgulho', 'paix√£o', 'parabenizar', 'paz', 'piedoso', 'positivo', 'prazenteiro', 'prazer', 'predile√ß√£o', 'preencher', 'prefer√™ncia', 'preferido', 'promissor', 'prosperidade', 'prote√ß√£o', 'proteger', 'revigorar', 'simp√°tico', 'vantajoso', 'protetor', 'risada', 'sobreviv√™ncia', 'vencedor', 'proveito', 'risonho', 'sobreviver', 'venera√ß√£o', 'provil√©gio', 'rom√¢ntico', 'sorte', 'ventura', 'querer', 'romantismo', 'sortudo', 'vida', 'radiante', 'saciar', 'sucesso', 'vigor', 'realizar', 'saci√°vel', 'surpreender', 'virtude', 'recomend√°vel', 'satisfa√ß√£o', 'tenro', 'virtuoso', 'reconhecer', 'satisfatoriamente', 'ternura', 'vit√≥ria', 'recompensa', 'satisfat√≥rio', 'torcer', 'vitorioso', 'recrear', 'satisfazer', 'tranquilo', 'viver', 'recreativo', 'satisfeito', 'tranquilo', 'vivo', 'recrea√ß√£o', 'sedu√ß√£o', 'triunfo', 'zelo', 'regozijar', 'seduzir', 'triunfal', 'zeloso', 'respeitar', 'sereno', 'triunfante', 'ressuscitar', 'simpaticamente', 'vantagem',]\n",
        "DESGOSTO = ['abomin√°vel', 'adoentado', 'amargamente', 'antipatia', 'antip√°tico', 'asco', 'asqueroso', 'avers√£o', 'chatear', 'chatea√ß√£o', 'desagrado', 'desagrad√°vel', 'desprez√≠vel', 'detest√°vel', 'doente', 'doen√ßa', 'enfermidade', 'enjoativo', 'enjoo', 'enj√¥o', 'feio', 'f√©tido', 'golfar', 'grave', 'gravidade', 'grosseiro', 'grosso', 'horr√≠vel', 'ign√≥bil', 'ilegal', 'incomodar', 'inc√¥mdo', 'indecente', 'indisposi√ß√£o', 'indisposto', 'inescrupuloso', 'maldade', 'maldoso', 'malvado', 'mau', 'nauseabundo', 'nauseante', 'nausear', 'nauseoso', 'nojento', 'nojo', 'n√°usea', 'obsceno', 'obstruir', 'obstru√ß√£o', 'ofensivo', 'pat√©tico', 'perigoso', 'repelente', 'repelir', 'repugnante', 'repulsa', 'repulsivo', 'repuls√£o', 'rude', 'sujeira', 'sujo', 'terrivelmente', 'terr√≠vel', 'torpe', 'travesso', 'travessura', 'ultrajante', 'vil', 'vomitar', 'v√¥mito',]\n",
        "MEDO = ['abomin√°vel', 'afugentar', 'alarmar', 'alerta', 'amea√ßa', 'amedrontar', 'angustia', 'ang√∫stia', 'angustiadamente', 'ansiedade', 'ansioso', 'apavorar', 'apreender', 'apreens√£o', 'apreensivo', 'arrepio', 'assombrado', 'assombro', 'assustado', 'assustadoramente', 'atemorizar', 'aterrorizante', 'brutal', 'calafrio', 'chocado', 'chocante', 'consternado', 'covarde', 'cruel', 'crueldade', 'cruelmente', 'cuidado', 'cuidadosamente', 'cuidadoso', 'defender', 'defensor', 'defesa', 'derrotar', 'desconfiado', 'desconfian√ßa', 'desencorajar', 'desespero', 'deter', 'envergonhado', 'escandalizado', 'escurid√£o', 'espantoso', 'estremecedor', 'estremecer', 'expulsar', 'feio', 'friamente', 'fugir', 'hesitar', 'horrendo', 'horripilante', 'horr√≠vel', 'horrivelmente', 'horror', 'horrorizar', 'impaci√™ncia', 'impaciente', 'impiedade', 'impiedoso', 'indecis√£o', 'inquieto', 'inseguran√ßa', 'inseguro', 'intimidar', 'medonho', 'medroso', 'monstruosamente', 'mortalha', 'nervoso', 'p√¢nico', 'pavor', 'premoni√ß√£o', 'preocupar', 'press√°gio', 'pressentimento', 'recear', 'receativamente', 'receio', 'receoso', 'ruim', 'suspeita', 'suspense', 'susto', 'temer', 'tenso', 'terror', 'tremor', 'temeroso', 'terrificar', 'timidamente', 'vigiar', 'temor', 'terr√≠vel', 'timidez', 'vigilante', 'tens√£o', 'terrivelmente', 't√≠mido',]\n",
        "RAIVA = ['abomina√ß√£o', 'aborrecer', 'adredido', 'agredir', 'agress√£o', 'agressivo', 'amaldi√ßoado', 'amargor', 'amargura', 'amolar', 'ang√∫stia', 'animosidade', 'antipatia', 'antip√°tico', 'asco', 'assassinar', 'assassinato', 'assediar', 'ass√©dio', 'atormentar', 'avarento', 'avareza', 'avers√£o', 'beligerante', 'bravejar', 'chatea√ß√£o', 'chato', 'cobi√ßoso', 'c√≥lera', 'col√©rico', 'complicar', 'contraiedade', 'contrariar', 'corrup√ß√£o', 'corrupto', 'cruxificar', 'demon√≠aco', 'dem√¥nio', 'descaso', 'descontente', 'descontrole', 'desenganar', 'desgostar', 'desgra√ßa', 'desprazer', 'desprezar', 'destrui√ß√£o', 'destruir', 'detestar', 'diabo', 'diab√≥lico', 'doido', 'encolerizar', 'energicamente', 'enfurecido', 'enfuriante', 'enlouquecer', 'enraivecer', 'escandalizar', 'esc√¢ndalo', 'escoriar', 'exasperar', 'execra√ß√£o', 'ferir', 'frustra√ß√£o', 'frustrar', 'f√∫ria', 'furioso', 'furor', 'gan√¢ncia', 'ganancioso', 'guerra', 'guerreador', 'guerrilha', 'hostil', 'humilhar', 'implic√¢ncia', 'implicar', 'importunar', 'incomodar', 'inc√¥modo', 'indignar', 'infernizar', 'inimigo', 'inimizade', 'inj√∫ria', 'injuriado', 'injusti√ßa', 'insulto', 'mal√≠cia', 'odi√°vel', 'repulsivo', 'inveja', 'malicioso', '√≥dio', 'resmungar', 'ira', 'malignidade', 'odioso', 'ressentido', 'irado', 'mal√≠gno', 'ofendido', 'revolta', 'irascibilidade', 'maltratar', 'ofensa', 'rid√≠culo', 'irasc√≠vel', 'maluco', 'opress√£o', 'tempestuoso', 'irritar', 'malvadeza', 'opressivo', 'tirano', 'louco', 'malvado', 'oprimir', 'tormento', 'loucura', 'matar', 'persegui√ß√£o', 'torturar', 'magoar', 'mesquinho', 'perseguir', 'ultrage', 'mal', 'misantropia', 'perturbar', 'ultrajar', 'maldade', 'misantr√≥pico', 'perverso', 'vexat√≥rio', 'maldi√ß√£o', 'molestar', 'provocar', 'vigoroso', 'maldito', 'mol√©stia', 'rabugento', 'vingan√ßa', 'maldizer', 'mortal', 'raivoso', 'vingar', 'maldoso', 'morte', 'rancor', 'vingativo', 'malefic√™ncia', 'mort√≠fero', 'reclamar', 'viol√™ncia', 'mal√©fico', 'mortificar', 'repress√£o', 'violento', 'malevol√™ncia', 'nervoso', 'reprimir', 'zangar', 'mal√©volo', 'odiar', 'repulsa',]\n",
        "SURPRESA = ['admirar', 'afei√ß√£o', 'apavorante', 'assombro', 'chocado', 'chocante', 'desconcertar', 'deslumbrar', 'embasbacar', 'emudecer', 'encantamento', 'enorme', 'espanto', 'estupefante', 'estupefato', 'estupefazer', 'expectativa', 'fantasticamente', 'fant√°stico', 'horripilante', 'imagin√°rio', 'imenso', 'impressionado', 'incr√≠vel', 'maravilha', 'milagre', 'mist√©rio', 'misterioso', '√≥timo', 'pasmo', 'perplexo', 'prod√≠gio', 'sensacional', 'surpreendente', 'surpreender', 'suspense', 'susto', 'temor', 'tremendo',]\n",
        "TRISTEZA = ['abandonar', 'abatido', 'abomin√°vel', 'aborrecer', 'abortar', 'afligir', 'aflito', 'afli√ß√£o', 'agoniar', 'amargo', 'amargor', 'amargura', 'ansiedade', 'arrepender', 'arrependidamente', 'atrito', 'azar', 'cabisbaixo', 'choro', 'choroso', 'chor√£o', 'coitado', 'compassivo', 'compun√ß√£o', 'contristador', 'contrito', 'contri√ß√£o', 'culpa', 'defeituoso', 'degradante', 'deplor√°vel', 'deposi√ß√£o', 'depravado', 'depressivo', 'depress√£o', 'deprimente', 'deprimir', 'derrota', 'derrubar', 'desalentar', 'desamparo', 'desanimar', 'desapontar', 'desconsolo', 'descontente', 'desculpas', 'desencorajar', 'desespero', 'desgaste', 'desgosto', 'desgra√ßa', 'desistir', 'desist√™ncia', 'deslocado', 'desmoralizar', 'desolar', 'desonra', 'despojado', 'desprazer', 'desprezo', 'desumano', 'des√¢nimo', 'discriminar', 'disforia', 'disf√≥rico', 'dissuadir', 'doloroso', 'dor', 'd√≥', 'enfadado', 'enlutar', 'entediado', 'entristecedor', 'entristecer', 'envergonhar', 'errante', 'erro', 'err√¥neo', 'escurecer', 'escurid√£o', 'escuro', 'esquecido', 'estragado', 'execr√°vel', 'extirpar', 'falsidade', 'falso', 'falta', 'fraco', 'fraqueza', 'fric√ß√£o', 'frieza', 'frio', 'funesto', 'f√∫nebre', 'grave', 'horror', 'humilhar', 'inconsol√°vel', 'indefeso', 'infelicidade', 'infeliz', 'infort√∫nio', 'isolar', 'lacrimejante', 'lacrimoso', 'lamentar', 'lastimoso', 'luto', 'lutoso', 'l√°grima', 'l√°stima', 'l√∫gubre', 'magoar', 'martirizar', 'mart√≠rio', 'mau', 'melancolia', 'melanc√≥lico', 'menosprezar', 'miseravelmente', 'misterioso', 'mist√©rio', 'mis√©ria', 'morre', 'morte', 'mortificante', 'm√°goa', 'negligentemente', 'nocivo', 'obscuro', 'opressivo', 'opress√£o', 'oprimir', 'pena', 'penalizar', 'penitente', 'penoso', 'penumbra', 'perder', 'perturbado', 'perverso', 'pervertar', 'pesaroso', 'pessimamente', 'piedade', 'pobre', 'porcamente', 'prejudicado', 'prejudicial', 'preju√≠zo', 'pressionar', 'press√£o', 'quebrar', 'queda', 'queixoso', 'recha√ßar', 'remorso', 'repressivo', 'repress√£o', 'reprimir', 'ruim', 'secreto', 'servil', 'sobrecarga', 'sobrecarregado', 'sofrer', 'sofrimento', 'solid√£o', 'sombrio', 'soturno', 'sujo', 'suplicar', 'supl√≠cio', 's√≥', 'timidez', 'torturar', 'trevas', 'triste', 'tristemente', 't√©dio', 't√≠mido', 'vazio',]\n",
        "\n",
        "emotion_words = {\n",
        "    'ALEGRIA': ALEGRIA,\n",
        "    'DESGOSTO': DESGOSTO,\n",
        "    'MEDO': MEDO,\n",
        "    'RAIVA': RAIVA,\n",
        "    'SURPRESA': SURPRESA,\n",
        "    'TRISTEZA': TRISTEZA,\n",
        "}\n",
        "for key, values in words.items():\n",
        "    for i, word in enumerate(values):\n",
        "        emotion_words[key][i] = ''.join([p.lemma_ for p in nlp(word.lower())])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Zf-PxyeIwi0g",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "d2118947-5183-486e-8261-a1e18cf0966d"
      },
      "cell_type": "code",
      "source": [
        "stpwords = set(stopwords.words('portuguese') + list(punctuation))\n",
        "# stpwords = set(list(punctuation))\n",
        "\n",
        "def tokenize_frases(frase):\n",
        "    return word_tokenize(frase.lower())\n",
        "\n",
        "def rm_stop_words_tokenized(frase):\n",
        "    frase = nlp(frase.lower())\n",
        "    clean_frase = []\n",
        "    for palavra in frase:\n",
        "        if palavra.pos_ != 'PUNCT':\n",
        "            palavra = palavra.lemma_\n",
        "            if palavra not in stpwords and not palavra.isdigit():\n",
        "                clean_frase.append(palavra)\n",
        "    return ' '.join(filter(None, clean_frase))\n",
        "\n",
        "def generate_corpus(frases, tokenize=False):\n",
        "    print('Iniciando processamento...')\n",
        "    tokenized_frases = frases\n",
        "    with concurrent.futures.ProcessPoolExecutor(max_workers=4) as procs:\n",
        "        if tokenize:\n",
        "            print('Executando processo de tokeniza√ß√£o das frases...')\n",
        "            tokenized_frases = procs.map(tokenize_frases, frases, chunksize=25)\n",
        "        print('Executando processo de remo√ß√£o das stopwords...')\n",
        "        tokenized_frases = procs.map(rm_stop_words_tokenized, tokenized_frases, chunksize=25)\n",
        "    print('Filtro e finaliza√ß√£o...')\n",
        "    return tokenized_frases\n",
        "\n",
        "\n",
        "frases = [\n",
        "    'Bom dia SENADOR, agora est√° claro porque o ped√°gio n√£o baixava,o judici√°rio n√£o se manifestava quando era provocado e as CPIs s√≥ serviram pr√° corrup√ß√£o,deu no que deu üôÑ',\n",
        "    'N√£o basta apenas retirar o candidato preferencial da maioria dos eleitores brasileiros. Tem que impedir tamb√©m que esses mesmos eleitores possam comparecer √†s urnas. Que democracia √© essa, minha gente? Poder judici√°rio comprometido at√© os cabelos com o golpe de destr√≥i o pa√≠s.',\n",
        "    'Deus aben√ßoe o dia de todos voc√™, tenham um bom trabalho e bom estudo a todos. E pra aqueles que n√£o trabalha e nem estuda, boa curti√ß√£o em sua cama üôÇ',\n",
        "    'Aprenda a ter amor pr√≥prio que nem essa banana q fez uma tatuagem dela mesma.',\n",
        "    'Estou muito feliz hoje',\n",
        "    'Dias chuvosos me deixam triste',\n",
        "    'Hoje o dia esta excelente',\n",
        "]\n",
        "\n",
        "N = 10000\n",
        "# frases = flt.sents()[:N] + mch.sents()[:N] + mcm.sents()[:N]\n",
        "\n",
        "frases = list(generate_corpus(frases, tokenize=False))\n",
        "print(frases)\n",
        "\n",
        "ldocs = [f'D{i}' for i in range(len(frases))]"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Iniciando processamento...\n",
            "Executando processo de remo√ß√£o das stopwords...\n",
            "Filtro e finaliza√ß√£o...\n",
            "['bom dia senador agora estar claro porque ped√°gio baixar judici√°rio manifestar ser provocar cpis servir pr√° corrup√ß√£o dar dar üôÑ', 'basto apenas retirar candidatar preferencial maioria eleitor brasileiro ter impedir eleitor poder comparecer s urna democracia ser gente poder judici√°rio comprometer cabelo golpe destruir pa√≠s', 'deus aben√ßoar dia todo ter bom trabalhar bom estudar todo pra trabalhar estudar bom curti√ß√£o suar cama üôÇ', 'aprender ter amor pr√≥prio banana q fazer umar tatuagem d', 'estar feliz hoje', 'dia chuvoso deixar triste', 'hoje dia excelente']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7CXkPtuVzio8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "15ae2736-b482-4d0d-d7be-9d31eecd4171"
      },
      "cell_type": "code",
      "source": [
        "print('Tf-Idf:')\n",
        "vectorizer = TfidfVectorizer(max_df=1, sublinear_tf=True, use_idf=True, ngram_range=(1, 1))\n",
        "X_tfidf = vectorizer.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_tfidf.get_shape()[1])\n",
        "weights_df = pd.DataFrame(np.round(X_tfidf.toarray().T, 3), index=vectorizer.get_feature_names(), columns=ldocs)\n",
        "display(weights_df.head(15))"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tf-Idf:\n",
            "   Actual number of tfidf features: 53\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aben√ßoar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amor</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>apenas</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aprender</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baixar</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>banana</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.378</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>basto</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brasileiro</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cabelo</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cama</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.262</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>candidatar</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chuvoso</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.577</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claro</th>\n",
              "      <td>0.259</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comparecer</th>\n",
              "      <td>0.000</td>\n",
              "      <td>0.21</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "               D0    D1     D2     D3   D4     D5   D6\n",
              "aben√ßoar    0.000  0.00  0.262  0.000  0.0  0.000  0.0\n",
              "agora       0.259  0.00  0.000  0.000  0.0  0.000  0.0\n",
              "amor        0.000  0.00  0.000  0.378  0.0  0.000  0.0\n",
              "apenas      0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "aprender    0.000  0.00  0.000  0.378  0.0  0.000  0.0\n",
              "baixar      0.259  0.00  0.000  0.000  0.0  0.000  0.0\n",
              "banana      0.000  0.00  0.000  0.378  0.0  0.000  0.0\n",
              "basto       0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "brasileiro  0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "cabelo      0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "cama        0.000  0.00  0.262  0.000  0.0  0.000  0.0\n",
              "candidatar  0.000  0.21  0.000  0.000  0.0  0.000  0.0\n",
              "chuvoso     0.000  0.00  0.000  0.000  0.0  0.577  0.0\n",
              "claro       0.259  0.00  0.000  0.000  0.0  0.000  0.0\n",
              "comparecer  0.000  0.21  0.000  0.000  0.0  0.000  0.0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "GbRzyK7Zn4G6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 556
        },
        "outputId": "f7386896-431b-447c-e1e9-a0f1973de73e"
      },
      "cell_type": "code",
      "source": [
        "print('Count:')\n",
        "vectorizer = CountVectorizer()\n",
        "X_count = vectorizer.fit_transform(frases)\n",
        "print(\"   Actual number of tfidf features: %d\" % X_count.get_shape()[1])\n",
        "weights_df = pd.DataFrame(X_count.toarray().T, index=vectorizer.get_feature_names(), columns=ldocs)\n",
        "display(weights_df.head(15))"
      ],
      "execution_count": 215,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Count:\n",
            "   Actual number of tfidf features: 60\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>D0</th>\n",
              "      <th>D1</th>\n",
              "      <th>D2</th>\n",
              "      <th>D3</th>\n",
              "      <th>D4</th>\n",
              "      <th>D5</th>\n",
              "      <th>D6</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>aben√ßoar</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>agora</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>amor</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>apenas</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>aprender</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>baixar</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>banana</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>basto</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>bom</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>brasileiro</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cabelo</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cama</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>candidatar</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>chuvoso</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>claro</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            D0  D1  D2  D3  D4  D5  D6\n",
              "aben√ßoar     0   0   1   0   0   0   0\n",
              "agora        1   0   0   0   0   0   0\n",
              "amor         0   0   0   1   0   0   0\n",
              "apenas       0   1   0   0   0   0   0\n",
              "aprender     0   0   0   1   0   0   0\n",
              "baixar       1   0   0   0   0   0   0\n",
              "banana       0   0   0   1   0   0   0\n",
              "basto        0   1   0   0   0   0   0\n",
              "bom          1   0   3   0   0   0   0\n",
              "brasileiro   0   1   0   0   0   0   0\n",
              "cabelo       0   1   0   0   0   0   0\n",
              "cama         0   0   1   0   0   0   0\n",
              "candidatar   0   1   0   0   0   0   0\n",
              "chuvoso      0   0   0   0   0   1   0\n",
              "claro        1   0   0   0   0   0   0"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "uvfJwFa8atEL",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "36487e22-fb07-4ec9-862b-81839c597485"
      },
      "cell_type": "code",
      "source": [
        "print('SVD: ')\n",
        "AC = copy.deepcopy(X_count.toarray().T)\n",
        "u, s, v = np.linalg.svd(AC, full_matrices=False)\n",
        "print('Original and SVD equals: ', np.allclose(AC, np.dot(u, np.dot(np.diag(s), v))))\n",
        "\n",
        "# print(AC)\n",
        "# print(u.astype(np.float16))\n",
        "# print('-' * 20)\n",
        "# print(np.diag(s.astype(np.float16)))\n",
        "# print('-' * 20)\n",
        "# print(v.astype(np.float16))"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "SVD: \n",
            "Original and SVD equals:  True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "tCgjE9sRbKls",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# hmm-lda\n",
        "# https://ieeexplore.ieee.org/document/7363382\n",
        "# https://link.springer.com/chapter/10.1007/978-3-642-21802-6_57\n",
        "# http://www.ppgia.pucpr.br/~paraiso/Projects/Emocoes/Emocoes.html\n",
        "\n",
        "emotion_weigths = {}\n",
        "for k, items in enumerate(emotion_words.items()):\n",
        "    key, values = items\n",
        "    emotion_weigths[key] = []\n",
        "    for i, word in enumerate(values):\n",
        "        idx_val = 0\n",
        "        w = 0\n",
        "        try:\n",
        "            index = weights_df.index.get_loc(word)\n",
        "            idx_val = u[index]\n",
        "            w = weights_df.iloc[index].values\n",
        "        except:\n",
        "            w = np.zeros((len(ldocs, )))\n",
        "        emotion_weigths[key].append(idx_val * w)\n",
        "# pp.pprint(emotion_weigths)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zctaT8xVmiJJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243
        },
        "outputId": "a3db493b-4ed4-4e2b-e1d2-b6a7a09c91f5"
      },
      "cell_type": "code",
      "source": [
        "dtframe = {d: np.zeros(len(ldocs)) for d in emotion_weigths}\n",
        "for k, item in enumerate(emotion_weigths.items()):\n",
        "    sent = np.array(item[1])\n",
        "    for i, m in enumerate(cosine_distances(v.T, sent)):\n",
        "        print(m)\n",
        "        dtframe[item[0]][i] = np.sum(m)\n",
        "\n",
        "# pp.pprint(dtframe)\n",
        "df = pd.DataFrame(list(dtframe.values()), index=dtframe.keys(), columns=ldocs)\n",
        "display(df.head(15))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-4e0c349dca09>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdtframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mldocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0memotion_weigths\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mitem\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0memotion_weigths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0msent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcosine_distances\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'emotion_weigths' is not defined"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "x7TwnB5okQkL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xyW_HKxkwURE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "9277f0ea-40b5-45a2-8f93-f6656898a770"
      },
      "cell_type": "code",
      "source": [
        "print(\"LSA using TruncatedSVD:\")\n",
        "\n",
        "# Project the tfidf vectors onto the first N principal components.\n",
        "# Though this is significantly fewer features than the original tfidf vector,\n",
        "# they are stronger features, and the accuracy is higher.\n",
        "svd = TruncatedSVD(50)\n",
        "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
        "\n",
        "# Run SVD on the training data, then project the training data.\n",
        "X_lsa = lsa.fit_transform(X_count)\n",
        "\n",
        "explained_variance = svd.explained_variance_ratio_.sum()\n",
        "print(\"   Explained variance of the SVD step: {}%\".format(int(explained_variance * 100)))\n",
        "\n",
        "print(svd.explained_variance_.shape)\n",
        "print(svd.singular_values_.shape) # S\n",
        "print(svd.components_.shape) # VT"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using TruncatedSVD:\n",
            "   Explained variance of the SVD step: 100%\n",
            "(4,)\n",
            "(4,)\n",
            "(4, 61)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ZHnVMukg0oWu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "008bfebd-37e4-4bc8-9530-f96c9d250313"
      },
      "cell_type": "code",
      "source": [
        "print('LSA using numpy:')\n",
        "u, s, v = np.linalg.svd(X_tfidf.toarray(), full_matrices=False)\n",
        "print(u)\n",
        "print(s.shape)\n",
        "print(v.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using numpy:\n",
            "[[0. 1. 0. 0.]\n",
            " [0. 0. 1. 0.]\n",
            " [0. 0. 0. 1.]\n",
            " [1. 0. 0. 0.]]\n",
            "(4,)\n",
            "(4, 58)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_KCInH7aaUQp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "9bc73216-ab28-4f39-c1ed-2a39a7af4076"
      },
      "cell_type": "code",
      "source": [
        "print('LSA using scikit-learn randomized_svd:')\n",
        "U, Sigma, VT = randomized_svd(X_count, \n",
        "                              n_components=50,\n",
        "                              n_iter=5,\n",
        "                              random_state=None)\n",
        "print(U.shape)\n",
        "print(Sigma.shape)\n",
        "print(VT.shape)\n",
        "\n",
        "print(U)\n",
        "# print(VT)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LSA using scikit-learn randomized_svd:\n",
            "(4, 4)\n",
            "(4,)\n",
            "(4, 61)\n",
            "[[ 8.23887410e-02  4.64153487e-01  8.81914756e-01  0.00000000e+00]\n",
            " [ 9.96228666e-01 -6.25206048e-02 -6.01632624e-02 -3.70560802e-16]\n",
            " [ 2.72128558e-02  8.83545536e-01 -4.67554003e-01  2.07365235e-16]\n",
            " [ 3.63520293e-16 -2.06384313e-16  7.46602991e-17  1.00000000e+00]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xy-B3w6obyOj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "b452f4f1-74d8-4410-937d-aeeff78b48f7"
      },
      "cell_type": "code",
      "source": [
        "# define a matrix\n",
        "# A = array([[1, 2], [3, 4], [5, 6]])\n",
        "A = np.array([\n",
        "    [1, 1, 1, 0, 0],\n",
        "    [3, 3, 3, 0, 0],\n",
        "    [4, 4, 4, 0, 0],\n",
        "    [5, 5, 5, 0, 0],\n",
        "    [0, 2, 0, 4, 4],\n",
        "    [0, 0, 0, 5, 5],\n",
        "    [0, 1, 0, 2, 2],\n",
        "])\n",
        "print(A.shape)\n",
        "\n",
        "\n",
        "u, s, v = np.linalg.svd(copy.deepcopy(A), full_matrices=False)\n",
        "\n",
        "print(u.astype(np.float16))\n",
        "print('-' * 20)\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print('-' * 20)\n",
        "print(v.astype(np.float16))"
      ],
      "execution_count": 195,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7, 5)\n",
            "[[-0.1376  -0.0236  -0.01081  0.56    -0.3757 ]\n",
            " [-0.4128  -0.07086 -0.03244  0.2064   0.756  ]\n",
            " [-0.5503  -0.0944  -0.04324 -0.7246  -0.1846 ]\n",
            " [-0.688   -0.11804 -0.05405  0.344   -0.2307 ]\n",
            " [-0.1528   0.5913   0.654    0.       0.2    ]\n",
            " [-0.0722   0.7314  -0.678    0.       0.     ]\n",
            " [-0.0764   0.2957   0.327    0.      -0.4    ]]\n",
            "--------------------\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "--------------------\n",
            "[[-0.5625  -0.593   -0.5625  -0.09015 -0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [-0.4097   0.8047  -0.4097  -0.09125 -0.09125]\n",
            " [-0.707    0.       0.707    0.       0.     ]\n",
            " [ 0.      -0.       0.      -0.707    0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0LFVgEqGyLRK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "99d47291-3c92-4804-a74f-3fd7c0174045"
      },
      "cell_type": "code",
      "source": [
        "u, s, v = randomized_svd(copy.deepcopy(A), \n",
        "                          power_iteration_normalizer='auto',\n",
        "                          flip_sign=True,\n",
        "                          n_components=100,\n",
        "                          n_iter=1,\n",
        "                          random_state=None)\n",
        "print(u.astype(np.float16))\n",
        "print('-' * 20)\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print('-' * 20)\n",
        "print(v.astype(np.float16))"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[ 1.3757e-01 -2.3605e-02  1.0811e-02  9.3652e-01  2.8711e-01]\n",
            " [ 4.1284e-01 -7.0862e-02  3.2440e-02 -3.1152e-01  8.4912e-01]\n",
            " [ 5.5029e-01 -9.4421e-02  4.3243e-02  1.2622e-01 -2.9834e-01]\n",
            " [ 6.8799e-01 -1.1804e-01  5.4047e-02 -1.0132e-01 -3.2812e-01]\n",
            " [ 1.5283e-01  5.9131e-01 -6.5381e-01 -1.5199e-04 -6.2406e-05]\n",
            " [ 7.2205e-02  7.3145e-01  6.7822e-01  0.0000e+00  0.0000e+00]\n",
            " [ 7.6416e-02  2.9565e-01 -3.2690e-01  3.0398e-04  1.2481e-04]]\n",
            "--------------------\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "--------------------\n",
            "[[ 0.5625   0.593    0.5625   0.09015  0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [ 0.4097  -0.8047   0.4097   0.09125  0.09125]\n",
            " [ 0.707    0.      -0.707   -0.      -0.     ]\n",
            " [-0.      -0.       0.       0.707   -0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "F4A5h4-8zNqm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "f91544f2-d7be-432e-dc8b-809779aea0dd"
      },
      "cell_type": "code",
      "source": [
        "u, s, v = svd(copy.deepcopy(A))\n",
        "print(u.astype(np.float16))\n",
        "print('-' * 20)\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print('-' * 20)\n",
        "print(v.astype(np.float16))"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.1376  -0.0236  -0.01081  0.56    -0.3757  -0.7     -0.1879 ]\n",
            " [-0.4128  -0.07086 -0.03244  0.2064   0.756   -0.258    0.378  ]\n",
            " [-0.5503  -0.0944  -0.04324 -0.7246  -0.1846  -0.344   -0.0923 ]\n",
            " [-0.688   -0.11804 -0.05405  0.344   -0.2307   0.57    -0.11536]\n",
            " [-0.1528   0.5913   0.654    0.       0.2      0.      -0.4    ]\n",
            " [-0.0722   0.7314  -0.678    0.       0.       0.       0.     ]\n",
            " [-0.0764   0.2957   0.327    0.      -0.4      0.       0.8    ]]\n",
            "--------------------\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "--------------------\n",
            "[[-0.5625  -0.593   -0.5625  -0.09015 -0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [-0.4097   0.8047  -0.4097  -0.09125 -0.09125]\n",
            " [-0.707    0.       0.707    0.       0.     ]\n",
            " [ 0.      -0.       0.      -0.707    0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EmepFJucztDU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "fe87e36b-0a46-4753-f535-97309438db11"
      },
      "cell_type": "code",
      "source": [
        "# SVD\n",
        "U, s, VT = svd(A)\n",
        "print(U.astype(np.float16))\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print(VT.astype(np.float16))"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.1376  -0.0236  -0.01081  0.56    -0.3757  -0.7     -0.1879 ]\n",
            " [-0.4128  -0.07086 -0.03244  0.2064   0.756   -0.258    0.378  ]\n",
            " [-0.5503  -0.0944  -0.04324 -0.7246  -0.1846  -0.344   -0.0923 ]\n",
            " [-0.688   -0.11804 -0.05405  0.344   -0.2307   0.57    -0.11536]\n",
            " [-0.1528   0.5913   0.654    0.       0.2      0.      -0.4    ]\n",
            " [-0.0722   0.7314  -0.678    0.       0.       0.       0.     ]\n",
            " [-0.0764   0.2957   0.327    0.      -0.4      0.       0.8    ]]\n",
            "[[12.484  0.     0.     0.     0.   ]\n",
            " [ 0.     9.51   0.     0.     0.   ]\n",
            " [ 0.     0.     1.346  0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]\n",
            " [ 0.     0.     0.     0.     0.   ]]\n",
            "[[-0.5625  -0.593   -0.5625  -0.09015 -0.09015]\n",
            " [-0.1266   0.02878 -0.1266   0.6953   0.6953 ]\n",
            " [-0.4097   0.8047  -0.4097  -0.09125 -0.09125]\n",
            " [-0.707    0.       0.707    0.       0.     ]\n",
            " [ 0.      -0.       0.      -0.707    0.707  ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RZoerdmc3Bbh",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "outputId": "b02defdb-2313-45ec-d392-fa827e97a3b6"
      },
      "cell_type": "code",
      "source": [
        "A = [\n",
        "    [1, 2],\n",
        "    [3, 4],\n",
        "    [5, 6],\n",
        "    [7, 8]\n",
        "]\n",
        "\n",
        "U, s, VT = svd(A)\n",
        "print(U.astype(np.float16))\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print(VT.T.astype(np.float16))"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.1525  -0.8228  -0.3945  -0.38   ]\n",
            " [-0.3499  -0.4214   0.2428   0.801  ]\n",
            " [-0.5474  -0.0201   0.6978  -0.4614 ]\n",
            " [-0.7446   0.381   -0.5464   0.04074]]\n",
            "[[14.266  0.   ]\n",
            " [ 0.     0.627]]\n",
            "[[-0.6416  0.767 ]\n",
            " [-0.767  -0.6416]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uK2Thy4WLSeo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "983b2701-163d-4d3d-afa2-a2982748d01a"
      },
      "cell_type": "code",
      "source": [
        "A = [\n",
        "    [2, 4],\n",
        "    [1, 3],\n",
        "    [0, 0],\n",
        "    [0, 0]\n",
        "    # [0, -1],\n",
        "    # [-2, 1],\n",
        "    # [1, 0]\n",
        "]\n",
        "\n",
        "U, s, VT = svd(A, full_matrices=False)\n",
        "print(U.astype(np.float16))\n",
        "print(np.diag(s.astype(np.float16)))\n",
        "print(VT.T.astype(np.float16))\n",
        "\n",
        "print(np.dot(U, U.T).astype(np.float16))\n",
        "print(np.dot(VT, VT.T).astype(np.float16))\n",
        "\n",
        "\n",
        "print(np.allclose(A, np.dot(U, np.dot(np.diag(s), VT))))\n",
        "\n",
        "print(np.dot(U, np.dot(np.diag(s), VT)).astype(np.float16))"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[-0.8174 -0.576 ]\n",
            " [-0.576   0.8174]\n",
            " [ 0.      0.    ]\n",
            " [ 0.      0.    ]]\n",
            "[[5.465 0.   ]\n",
            " [0.    0.366]]\n",
            "[[-0.4045 -0.9146]\n",
            " [-0.9146  0.4045]]\n",
            "[[ 1. -0.  0.  0.]\n",
            " [-0.  1.  0.  0.]\n",
            " [ 0.  0.  0.  0.]\n",
            " [ 0.  0.  0.  0.]]\n",
            "[[ 1. -0.]\n",
            " [-0.  1.]]\n",
            "True\n",
            "[[2. 4.]\n",
            " [1. 3.]\n",
            " [0. 0.]\n",
            " [0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GT2PeZ_bQH0J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}